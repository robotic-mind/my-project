# -*- coding: utf-8 -*-
"""project_code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1K4-je3tp3DRCR0WtkMaP_nZmZe0oBHMn

Let us import some necessary libraries
"""

import numpy as np
# for scientific computing, provides n-dimensional array
import pandas as pd
# for data analysis and manupulation
from sklearn.preprocessing import StandardScaler
# for standardisation of data
from matplotlib import pyplot as plt
# for data visualization
import seaborn as sns
# for data visualization

# uploading the csv file to Google Colab notebook
df=pd.read_csv("./dataset/synthetic_sessions_r.csv")

"""#  ***STEP 1**- EXPLORATORY DATA ANALYSIS AND DATA CLEANING*"""

# displaying total number of elements in the dataframe
df.size

# displaying the shape of the dataframe (rows x columns)
df.shape

"""No of rows= 12000

No of columns/features= 21
"""

# displaying basic information about the dataframe
df.info()

# displaying the first 20 rows of the dataframe
df.head(n=20)

# displaying the last 20 rows of the dataframe
df.tail(n=20)

# summary of the data
df.describe().T

# displaying duplicate rows in the dataset
df.duplicated(keep='first')

# displaying total number of duplicate rows
df.duplicated(keep='first').sum()

"""It is evident from the above output that the dataset contains 0 duplicate rows."""

# displaying missing values in the dataframe
df.isnull()

# calculating total number of missing values column wise
df.isnull().sum()

# calculating total number of missing values in the entire dataframe
df.isnull().sum().sum()

"""It is evident from the above output that the dataset contains 0 missing values"""

# displaying categorical features only
df.select_dtypes(include=['object']).columns.tolist()

# displaying numerical features only
df.select_dtypes(include=['int64','float64']).columns.tolist()

# calculating the total no of observations for each category in the column 'cluster'
df['cluster'].value_counts()

# extracting the unique values in the column 'cluster'
cluster_options=df['cluster'].unique().tolist()
print(cluster_options)

# computing correlation between each pair of features using pearson formula
corr_matrix=df.corr(method='pearson',numeric_only=True)

# displaying correlation of each feature with the target variable
corr_matrix['is_anomaly']

# visualizing correlation matrix
plt.figure(figsize=(15,6))
sns.heatmap(corr_matrix,annot=True,cmap='coolwarm',fmt='.2f')

"""It is evident from the above correlation heatmap that there are 9 features/columns that have a correlation value of 0 which means they do not affect the target variable. Following are the list of columns:


1.   session_id
2.   user_id


1.   location_accuracy_m
2.   login_hour

1.   session_duration_min
2.   time_since_last_login_min

1.   total_usage_min
2.   mouse_acceleration

1.   flight_time

Let us drop these columns from the dataframe and retain the ones that impact the target variable










"""

# creating a copy of the original dataframe df for back up
df_copy=df.copy(deep=True)

df_copy.shape

df_copy.columns

# creating a list of columns to be dropped
columns_dropped=['session_id','user_id','location_accuracy_m','login_hour','session_duration_min','time_since_last_login_min','total_usage_min','mouse_acceleration','flight_time']

columns_dropped

# dropping columns that have no impact on the target variable from the original dataframe
df.drop(columns=columns_dropped,axis=1,inplace=True)

df.size

df.shape

"""After dropping the redundant features, the dataframe now has 12000 rows and 12 columns."""

df.columns.tolist()

"""Now let us separate the dataframe into independent(X) and dependent(y) features."""

# separate X (independent features)
X=df.iloc[:,:-1]
# separate y (target variable)
y=df.iloc[:,-1]

X.columns.tolist()

X.shape

type(X)

"""X (independent features) is a pandas dataframe."""

y

type(y)

"""y (target variable) is a pandas series

# Data Visualization

Let us compare each independent feature with the target variable to gain a better understanding of the dataset through graphical representations
"""

# average key press durations vs is_anomaly
sns.violinplot(x=y,y=X['avg_key_press_duration'])
plt.title("average key press durations vs is_anomaly")

# error_rate vs is_anomaly
sns.violinplot(x=y,y=X['error_rate'])
plt.title("error_rate vs is_anomaly")
plt.figure(figsize=(8,6))

# typing speed characters per second vs is_anomaly
sns.violinplot(x=y,y=X['typing_speed_chars_per_sec'])
plt.title("typing speed characters per second vs is_anomaly")

# mouse_avg_speed vs is_anomaly
sns.violinplot(x=y,y=X['mouse_avg_speed'])
plt.title("mouse_avg_speed vs is_anomaly")

# click_rate vs is_anomaly
sns.violinplot(x=y,y=X['click_rate'])
plt.title("click_rate vs is_anomaly")

# scroll_speed vs is_anomaly
sns.violinplot(x=y,y=X['scroll_speed'])
plt.title("scroll_speed vs is_anomaly")

# latitude vs is_anomaly
sns.violinplot(x=y,y=X['latitude'])
plt.title("latitude vs is_anomaly")

# longitude vs is_anomaly
sns.violinplot(x=y,y=X['longitude'])
plt.title("longitude vs is_anomaly")

# num_active_apps vs is_anomaly
sns.violinplot(x=y,y=X['num_active_apps'])
plt.title("num_active_apps vs is_anomaly")

# foreground_switch_count vs is_anomaly
sns.violinplot(x=y,y=X['foreground_switch_count'])
plt.title("foreground_switch_count vs is_anomaly")

"""# *STEP 2- DATA PREPROCESSING*

Let us import some libraries.
"""

from sklearn.preprocessing import OneHotEncoder
# for converting nominal categorical values into numerical values
from sklearn.preprocessing import StandardScaler
# for standardizing features
from sklearn.compose import ColumnTransformer
# allows us to apply different transformations to different subsets of columns
# within a single,consistent workflow

"""#  Data Encoding


"""

# define categorical features
categorical_features=X.select_dtypes(include=['object']).columns.tolist()

categorical_features

# define numerical features
numerical_features=X.select_dtypes(include=['int64','float64']).columns.tolist()

numerical_features

# initializing OneHotEncoder
encoder=OneHotEncoder(handle_unknown='ignore',sparse_output=False)
# handle_unknown='ignore'--> skips throwing an error when it encounters new,
# unseen categorical values in the test/production data which were not present
# in the training data. The resulting one-hot encoded columns for that specific
# category will be set to all zeros.

"""# Feature Scaling
Now let us standardize X (dataframe) using StandardScaler so that each feature has a mean of 0 and a standard deviation of 1.
"""

# initializing StandardScaler
scaler=StandardScaler()

# bundle preprocessing for numerical and categorical data
preprocessor=ColumnTransformer(transformers=[('scale',scaler,numerical_features),('encode',encoder,categorical_features)],remainder='passthrough')

"""# Train Test Split
Now let us split X (independent features) and y (target variable) into train data and test data
"""

from sklearn.model_selection import train_test_split
# for splitting the dataframes into train data and test data

X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=42)

X_train.shape

X_train.info()

X_test.shape

y_train.shape

y_test.shape

type(X_train)

type(y_train)

# transform training data
X_train_scaled=preprocessor.fit_transform(X_train)

X_train_scaled

"""The **same** mean and standard deviation values calculated from the *training data* (X_train) are used to transform or scale the *test data* (X_test) or any future data. This prevents **data leakage** from the test data into the model training process."""

# apply the transformation to X_test
X_test_scaled=preprocessor.transform(X_test)

X_test_scaled

X_train_scaled.shape

X_test_scaled.shape

"""# *STEP 3- MODEL CREATION*

# Naive Bayes
"""

from sklearn.naive_bayes import GaussianNB

# build a Gaussian classifier
gnb=GaussianNB()
# train the model
gnb.fit(X_train_scaled,y_train)

# make predictions
pred_gnb=gnb.predict(X_test_scaled)

pred_gnb

"""# Logistic Regression"""

from sklearn.linear_model import LogisticRegression

# initialize model
clf=LogisticRegression(max_iter=10000,random_state=0)
# train the model
clf.fit(X_train_scaled,y_train)

# make predictions
pred_logistic_reg=clf.predict(X_test_scaled)

pred_logistic_reg

"""# Random Forest"""

from sklearn.ensemble import RandomForestClassifier

# initialize model
rf_classifier=RandomForestClassifier(n_estimators=100,random_state=42)
# train the model
rf_classifier.fit(X_train_scaled,y_train)

# make predictions
pred_rf=rf_classifier.predict(X_test_scaled)

pred_rf

"""# *STEP 4- MODEL EVALUATION*

Let us import some libraries
"""

from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score,confusion_matrix,ConfusionMatrixDisplay
from sklearn.metrics import roc_curve,auc

"""# For Naive Bayes Model"""

# accuracy score
score_gnb=accuracy_score(y_test,pred_gnb)
print("Accuracy score for Naive Bayes model= ",score_gnb*100)

# precision score
precision_gnb=precision_score(y_test,pred_gnb,average='binary')
# 'average' required for binary/multiclass
print("Precision score for Naive Bayes model= ",precision_gnb*100)

# recall score
recall_gnb=recall_score(y_test,pred_gnb,average='binary')
print("Recall score for Naive Bayes model= ",recall_gnb*100)

# f1 score
f1_gnb=f1_score(y_test,pred_gnb,average='binary')
print("F1 score for Naive Bayes model= ",f1_gnb*100)

# confusion matrix
cm_gnb=confusion_matrix(y_test,pred_gnb)

# display confusion matrix
display3=ConfusionMatrixDisplay(confusion_matrix=cm_gnb,display_labels=['Anomaly','Not Anomaly'])
display3.plot()
plt.title("Confusion Matrix",pad=30)
plt.xlabel("Predicted values")
plt.ylabel("Actual values")
plt.gca().xaxis.set_label_position('top')# 'gca'- get current axes
plt.gca().xaxis.tick_top()
plt.show()

"""From the above confusion matrix we can conclude-



*   True Positive= 822

*   False Negative= 137
*   False Positive= 479


*   True Negative= 2162

# for Logistic Regression Model
"""

# accuracy score
score_lr=accuracy_score(y_test,pred_logistic_reg)
print("Accuracy score for logistic regression model= ",score_lr*100)

# precision score
precision_lr=precision_score(y_test,pred_logistic_reg,average='binary')
# 'average' required for binary/multiclass
print("Precision score for logistic regression model= ",precision_lr*100)

# recall score
recall_lr=recall_score(y_test,pred_logistic_reg,average='binary')
print("Recall score for logistic regression model= ",recall_lr*100)

# F1 score
f1_lr=f1_score(y_test,pred_logistic_reg,average='binary')
print("F1 score for logistic regression model= ",f1_lr*100)

# confusion matrix
cm_lr=confusion_matrix(y_test,pred_logistic_reg)

# display confusion matrix
display1=ConfusionMatrixDisplay(confusion_matrix=cm_lr,display_labels=['Anomaly','Not Anomaly'])
display1.plot()
plt.title("Confusion Matrix",pad=30)
plt.xlabel("Predicted values")
plt.ylabel("Actual values")
plt.gca().xaxis.set_label_position('top')# 'gca'- get current axes
plt.gca().xaxis.tick_top()
plt.show()

"""From the above confusion matrix we can conclude-



*   True Positive= 788

*   False Negative= 171
*   False Positive= 183


*   True Negative= 2458

# For Random Forest Classifier Model
"""

# accuracy score
score_rf=accuracy_score(y_test,pred_rf)
print("Accuracy score for Random Forest model= ",score_rf*100)

# precision score
precision_rf=precision_score(y_test,pred_rf,average='binary')
# 'average' required for binary/multiclass
print("Precision score for Random Forest model= ",precision_rf*100)

# recall score
recall_rf=recall_score(y_test,pred_rf,average='binary')
print("Recall score for Random Forest model= ",recall_rf*100)

# F1 score
f1_rf=f1_score(y_test,pred_rf,average='binary')
print("F1 score for Random Forest model= ",f1_rf*100)

# confusion matrix
cm_rf=confusion_matrix(y_test,pred_rf)

# display confusion matrix
display2=ConfusionMatrixDisplay(confusion_matrix=cm_rf,display_labels=['Anomaly','Not Anomaly'])
display2.plot()
plt.title("Confusion Matrix",pad=30)
plt.xlabel("Predicted values")
plt.ylabel("Actual values")
plt.gca().xaxis.set_label_position('top')# 'gca'- get current axes
plt.gca().xaxis.tick_top()
plt.show()

"""From the above confusion matrix we can conclude-



*   True Positive= 759

*   False Negative= 200
*   False Positive= 177


*   True Negative= 2464

# Logistic Regression Model vs Random Forest Classifier Model vs Naive Bayes Model

We will use a bar plot to compare different performance metrics of the 3 models.
"""

data={
    'Model':['Logistic Regression','Random Forest','Naive Bayes','Logistic Regression','Random Forest','Naive Bayes','Logistic Regression','Random Forest','Naive Bayes','Logistic Regression','Random Forest','Naive Bayes'],
    'Metric':['Accuracy Score','Accuracy Score','Accuracy Score','Precision','Precision','Precision','Recall','Recall','Recall','F1 Score','F1 Score','F1 Score'],
    'Score':[score_lr,score_rf,score_gnb,precision_lr,precision_rf,precision_gnb,recall_lr,recall_rf,recall_gnb,f1_lr,f1_rf,f1_gnb]
}

# converting 'data' to a pandas dataframe
data_frame=pd.DataFrame(data)

data_frame

# Visualize using bar graph
plt.figure(figsize=(8,6))
ax=sns.barplot(x='Metric',y='Score',hue='Model',data=data_frame)
# we save the Axes object returned by the sns function in the 'ax' variable
ax.set_ylim(0.0,1.0)
ax.set_title("Comparison of various ML model performance metrics")
ax.set_xlabel("Performance Metric")
ax.set_ylabel("Score Value")
ax.legend(
    loc='center left',      # Anchor the legend's center-left point
    bbox_to_anchor=(1, 0.5), # to the point (1, 0.5) in the figure coordinates
    title='Model Name'       # Re-specify the title if needed
)

"""'hue' tells Seaborn to use different colors (hues) for each unique value in the 'Model' column, automatically creating the side-by-side (grouped/clustered) bars for the different models.

Let us plot the ROC-AUC graph to understand which model is performing better at distinguishing between the 2 classes.
"""

# creating a dataframe to store the true labels and the predicted
# probabilities from both of the ML models
test_df=pd.DataFrame({'true':y_test,'logistic':pred_logistic_reg,'RandomForest':pred_rf,'Naive_Bayes':pred_gnb})

test_df.head()

"""Now we will calculate the **True Positive Rate**, **False Positive Rate** and **AUC(Area under curve) score** and plot the graph."""

plt.figure(figsize=(8,6))
for model in ['logistic','RandomForest','Naive_Bayes']:
  fpr,tpr,_=roc_curve(test_df['true'],test_df[model])
  # '_' ignores the threshold values being returned
  auc_score=auc(fpr,tpr)
  plt.plot(fpr,tpr,label=f"{model} (AUC={auc_score:.2f})")

plt.plot([0,1],[0,1],'r--',label='Random Guess')

plt.title("ROC curves for two ML models")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.legend()
# displays the legend box using the labels defined using plt.plot()
plt.show()

"""From the above ROC-AUC graph we can infer that the logistic regression model delivers the better performance. This is because of the following 2 reasons:



1.   The Logistic Regression model curve is closer to the top-left corner of the graph than the Random Forest model curve and Naive Bayes model curve.
2.   The AUC score of Logistic Regression model is highest among the three models.

Hence we will select the logistic regression model for the purpose of future predictions and deployment.

# *STEP 5- SAVE CHOSEN MODEL FOR DEPLOYMENT*

Let us import some libraries.
"""

from sklearn.pipeline import Pipeline
import joblib

# extracting all the columns from X
columns_list=X.columns.tolist()

print(columns_list)

# save the unique values in column 'cluster'
joblib.dump(cluster_options,'cluster_options.pkl')

# create the full pipeline including the model
model_pipeline=Pipeline(steps=[('preprocessor',preprocessor),('classifier',clf)])

# save the list of columns
joblib.dump(columns_list,'columns_list.pkl')

# save the entire pipeline
joblib.dump(model_pipeline,'model_pipeline.pkl')